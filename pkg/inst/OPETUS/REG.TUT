SURVO 84C SUCROS@   7@REG1         153@REG2        1495@REG3        8099@REG4       13931@REG5       18977@REG6       21090@REG7       25512@TÿTa1@SCRATCH €   µµµµ Lineaarinen regressioanalyysi: Yleist„    Regressioanalyysissa tutkitaan yhden tilastollisen muuttujan Yriippuvuutta yhdest„ tai useammasta muusta muuttujasta X1,X2,...olettaen, ett„ Y-arvot voidaan lausua likim„„r„isen„ X-muuttujienfunktiona. Useimmiten riippuvuus oletetaan lineaariseksi eli oletetaan,ett„ Y on likimain muotoa a0+a1*X1+a2*X2+..., miss„ a0,a1,a2,...ovat tuntemattomia vakioita (regressiokertoimia). T„ll”in puhutaanlineaarisesta regressioanalyysista. Muuttujaa Y sanotaan selitett„v„ksimuuttujaksi ja muuttujia X1,X2,... selitt„viksi muuttujiksi.   T„rkein lineaarisen regression erikoistapaus on yhden selitt„j„nmalli eli Y likimain muotoa a0+a1*X, jolloin riippuvuutta voi kuvataXY-tasossa suoralla. Toinen merkitt„v„ erikoistapaus on polynomi-regressiomalli jossa regressiofunktio on esim. muotoaa0+a1*X+a2*X^2+a3*X^3 (eli kolmannen asteen polynomi). Kummastakinn„ist„ erikoistapauksista k„ytet„„n my”s nimityst„ k„yr„nsovitus.   Kaikkia lineaarisen regressioanalyysin muotoja k„sitell„„n SurvossaLINREG-operaatiolla. Laajassa Survossa on k„ytett„viss„ muitakinoperaatioita, mm. REGDIAG ns. regressiodiagnostiikkaan ja ESTIMATEyleiseen ep„lineaariseen regressioanalyysiin.Tp        T„m„ sukro p„„ttyy. Paina ENTER-nappia!@TVC1200@_@GA@TXA@Tp@0T!1@C2@Ta-1@TLREGS@TÿTa1@SCRATCH€üGPLOT /DEL ALL€Bü   µµµµ Johdatteleva esimerkki Ta-1@TW5@   Saadaksemme k„sityksen regressioanalyysin tekniikasta ja LINREG-operaation k„yt”st„ tutkimme asiaa ensin pienten keinotekoistenaineistojen avulla.TW20@   T„ss„ on kahden muuttujan X,Y ja 6 havainnon taulukko:TW10@DATA KOETW5@ X  YTW5@ 1  5TW5@ 2  7 3  9 4 11 5 13 6 15TW10@   Huomaamme heti,TW5@ ett„ jokaiselle havainnolle p„tee Y=2*X+3TW10@ elihavaintopisteet (X,Y) ovat samalla suoralla.TW10@   T„m„ ilmenee my”s seuraavasta kuvasta:TW10@GPLOT KOE,X,Y / XSCALE=0(1)7 YSCALE=0(2)16TW10@€TW10@.±   Katsomme nyt,TW5@ mit„ LINREG-operaatio sanoo t„st„,TW5@ kun laskemme sellai-sen regressiomallin,TW5@ jossa Y on selitett„v„ ja X selitt„v„ muuttuja.TW20@   Mallin rakenne kuvataan aina aktivoimalla selitett„v„ muuttujaY-kirjaimellaTW5@ ja selitt„v„t muuttujat X-kirjaimilla.TW10@ Sen voi tehd„VARS-t„smennyksell„TW5@VARS=X(X),Y(Y) .TW10@Huomaa, ett„ merkinn„ss„ X(X) ensimm„inen X tarkoittaa muuttujan nime„TW5@ja suluissa oleva X on aktivointimerkint„.TW10@   Tulokset saadaan nyt k„ynnist„m„ll„ komentoTW5@LINREG KOE,0RT=1@1@+@C1@=TW10@TW10@€TW50@   Tuloslistan alussa annetaan mukana olevien muuttujien keskiarvot,hajonnat ja korrelaatiot.TW10@ Muuttujien X ja Y t„ydellisest„ lineaarisestariippuvuudesta seuraa,TW5@ ett„ niiden korrelaatiokerroin on 1.TW10@   Listan lopussa on varsinaisen regressioanalyysin tulokset.TW10@ Muuttu-jittain annetaan regressiokertoimet (Regr.coeff.)TW5@ ja niiden keskivir-heet (Std.dev.).TW10@ Nyt saimme muuttujan X regressiokertoimeksi 2TW5@ javakiotermiksi (constant) 3TW5@ eli regressioyht„l” on Y=2*X+3.TW10@   Koska riippuvuus oli t„ydellist„,TW5@ keskivirheet ovat nollia.TW10@Muilla tuloksilla ei t„ss„ tilanteessa ole mit„„n merkityst„.TW10@.±   "Huononnamme" aineistoa muuttamalla paria Y-arvoa.TW10@DATA KOE2 X   Y 1   5 2   7 3   9 4  11 5  13 6  15TW10@   Teemme seuraavat "korjaukset":TW10@TW10@10TW10@TW10@ 9TW10@TW5@TW5@   Havaintopisteet eiv„t en„„ ole samalla suoralla:TW10@GPLOT KOE2,X,Y / XSCALE=0(1)7 YSCALE=0(2)16TW10@TW10@€TW10@.±   LINREG antaa nyt tuloksetTW10@LINREG KOE2,0RT=1@1@+@C1@= / VARS=X(X),Y(Y)TW10@€TW20@0C.wWwC.wWw   Muuttujan X regressiokerroin on nyt = aikaisemman arvon 2asemastaTW5@ ja vakiotermi on @3 alkuper„isen arvon 3 sijasta.TW10@Sit„, ett„ yhteensopivuus ei ole en„„ ehdoton, kuvastavat keskivirheet@2 ja @4 pelkkien nollien asemasta.TW10@   Saatu regressiosuora Y==*X+@3 voitaisiin piirt„„omalla GPLOT-komennolla hajontakuvan p„„lle.TW10@ T„m„ tehd„„n kuitenkinyksinkertaisimmin liitt„m„ll„ hajontakuvan piirtoon TREND-t„smennys:TW10@.±GPLOT KOE2,X,Y / XSCALE=0(1)7 YSCALE=0(2)16 TREND=0TW10@€TW10@   Regressiosuora on sovitettu kulkemaan niin,TW5@ ett„ se mahdollisimmanhyvin yhtyy pistekuvioon.TW10@ On erilaisia periaatteita,TW5@ mill„ ko. sovitustapahtuu.TW10@ Hyvin yleisesti ja my”s t„ss„ k„ytet„„n ns. pienimm„n neli”-summan keinoa.TW10@.±   N„yt„mme, mist„ pienimm„n neli”summan keinossa on kysymys, laske-malla regressioanalyysin uudelleen siten,TW5@ ett„ LINREG m„„r„„my”s mallin ns. j„„nn”kset (residuaalit)TW5@ ja mallin antamat ennusteetkullekin havainnolle.TW10@ T„t„ varten m„„rittelemme aineiston siten,TW5@ ett„taulukolla on my”s maskirivi.TW10@DATA KOE3,A,B,N,M X   Y 1   5 2   7 3  10 4   9 5  13 6  15TW10@BANM 1  11TW10@   Ennen LINREG-operaation k„ynnistyst„ lis„„mme taulukkoon sarakkeetEnnuste ja J„„nn”s maskeineen.TW10@TW10@EnnusteTW5@  J„„nn”sTW5@11.111TW5@   11.111TW10@   Kun malliin valitaan muuttujat,TW5@ ennustemuuttujaa merkit„„n maskillaP (Prediction)TW5@ ja j„„nn”smuuttujaa maskilla R (Residual).TW10@   LINREG aktivoidaan muodossaTW5@LINREG KOE3  /  VARS=X(X),Y(Y),Ennuste(P),J„„nn”s(R)TW10@€TW40@   Emme ottaneet tulostaulukkoa lainkaan kentt„„n,TW5@ koska se on joedell„.TW10@ LINREG on kuitenkin t„ytt„nyt Ennuste- ja J„„nn”s-sarakkeet.TW30@   Mallin antamat ennusteet lasketaan kaavasta E=a*X+b ,TW10@ miss„a on muuttujan X regressiokerroin a==TW10@ ja b vakiotermib=@3 .TW10@ Siis esim. kolmannessa havainnossa X=3TW10@ ja E=€TW10@J„„nn”kset ovat Y-arvojen ja ennusteiden erotuksia J=Y-E .TW10@ Esim.kolmannessa havainnossa Y=10 ja J=€º .TW20@TW5@TW5@TW5@TW10@   Pienimm„n neli”summan keinossa parametrit a ja b on valittu siten,TW5@ett„ j„„nn”sten E neli”summa on mahdollisimman pieni.TW10@ Lopputuloksessaitse j„„nn”sten summa on aina 0.TW10@ (Tarkista vaikka kosketuslaskennalla.)TW10@Havaintojen 3 ja 4 tahallinen v„„rist„minen n„kyy nyt n„iden kohdallamuita suurempina j„„nn”ksin„.TW10@ Laskemme j„„nn”sneli”summan kosketus-laskennalla.TW10@ Se onTW5@´·+*€=wº .TW10@ J„„nn”sneli”summa esiintyy LINREG-tulostaulukossa vapausasteilla jaettuna j„„nn”svarianssina.TW10@Tp        Poimimme osan tulostaulukosta!@TW20@TW10@Tp@¹TW5@¹¹TW10@TW5@¹TW10@C= w=C= w=C= w=TW20@   J„„nn”svarianssi (Residual variance) on siis @6TW10@ ja senvapausasteet (df) 4TW10@ eli havaintojen m„„r„ 6 v„hennettyn„ estimoitujenparametrien (a,b) lukum„„r„ll„ 2.TW10@ J„„nn”sneli”summa saadaan n„in my”stulona 4*@6=€º .TW20@   Tuloksissa on my”s yhteiskorrelaatiokerroin (R=@7)TW10@ ja sen neli”(R^2=@8),TW10@ jotka mittaavat mallin kyky„ kuvata muuttujan Y vaihte-lua.TW10@ Erityisesti 100*R^2=T=9@C100@*@8@@9 kertoo,TW5@ kuinka monta prosenttiamalli selitt„„ muuttujan Y varianssista.TW10@.±   Piirr„mme lopuksi hajontakuvan niin,TW5@ ett„ mallin j„„nn”kset n„kyv„tsiin„ paksuina janoina.TW10@GPLOT KOE3,X,Y  /  XSCALE=0(1)7 YSCALE=0(2)16 TREND=[viivanleveys(1)],0TW10@POINT=[MUSTA],3,10 LINE=6 LINE2=[PUNAINEN][viivanleveys(3)],X,EnnusteTW10@TW10@€TW10@Tp        T„m„ sukro p„„ttyy. Paina ENTER-nappia!@TVC1200@_@@Ta1@BGPLOT /DEL ALL€Ta-1@Tp@0T!1@C3@TLREGS@Tÿ

Ta1@SCRATCH€üGPLOT /DEL ALL€Bü   µµµµ Paikan korkeuden m„„r„„minen veden kiehumispisteen avulla    Skotlantilainen fyysikko James D. Forbes tutki 1800-luvun puoli-v„liss„, miten veden kiehumispiste riippuu ilmanpaineesta.K„yt„nn”n tavoitteena oli saada aikaan yksinkertainen mittaustapapaikan korkeudelle merenpinnasta, sill„ olihan hyvin tiedossa, mitenilmanpaine alenee korkeuden kasvaessa. Ilmanpaineen mittaamiseenk„ytetyt instrumentit, barometrit elohopeaputkineen, olivat kuitenkintuohon aikaan varsin arkoja esineit„ kuljettaa matkoilla mukana.On helpompi l„mmitt„„ vett„ kattilassa ja mitata veden kiehumispiste.   Forbes tutki asiaa sek„ teoreettisesti ett„ kokeellisesti. H„nm„„ritti ilmanpaineen (P) ja kiehumispisteen (T) 17 eri paikassaAlpeilla ja Skotlannissa. H„nell„ oli teoriansa mukaan k„sitys, ett„riippuvuus on muotoa P=10^(a*T+b), miss„ vakiot a ja b tuli m„„r„t„koeaineiston perusteella.   Kuten kohta n„hd„„n, havainnot eiv„t olleet aivan tarkkoja. Niihinsis„ltyy mittausvirhett„. T„ll”in on luonnollista k„ytt„„ regressio-analyysia parametrien a ja b arviointiin. Vaikka riippuvuus ei alunperinole lineaarista, se voidaan palauttaa lineaariseksi logaritmoimalla,jolloin   lgt(P)=a*T+b   (lgt on t„ss„ 10-kantainen logaritmi).Tp        Jatka painamalla ENTER!@TVC1200@_@GA@TXA@Tp@.±Ta-1@   Aineisto, jonka Forbes ker„si, on seuraava.TW10@ Annamme tiedot alkuper„i-siss„ yksik”iss„TW5@ eli T Fahrenheit-asteinaTW5@ ja P elohopeatuumina!TW10@DATA FORBESTW5@Nro    T     PTW5@  1  194.5  20.79TW5@  2  194.3  20.79TW5@  3  197.9  22.40Ta1@  4  198.4  22.67  5  199.4  23.15  6  199.9  23.35  7  200.9  23.89  8  201.1  23.99  9  201.4  24.02 10  201.3  24.01 11  203.6  25.14 12  204.6  26.57 13  209.5  28.49 14  208.6  27.76 15  210.7  29.04 16  211.9  29.88 17  212.2  30.06Ta-1@TW20@   Koska muunnamme muuttujia,TW5@ varustamme havaintotaulukon FORBES my”srivitunnuksin ja maskein.TW10@TW5@TW5@ºTW5@,A,B,N,MTW10@TW5@MTW5@ 11  111.1  11.11TW10@TW5@NATW5@BTW5@TW5@TW5@TW5@TW5@   M„„rittelemme ja laskemme uuden muuttujan LP=100*log(P)/log(10)TW10@eli ilmanpaineen 100-kertaisen 10-kantaisen logaritmin:TW10@VAR LP TO FORBESTW10@TW10@LPTW10@TW5@111.111TW10@TW10@€TW30@   Tarkistamme tilanteen alustavasti kuvasta:TW10@GPLOT FORBES,T,LP  /  TREND=0TW10@€TW10@   Regressiosuora tuntuu sopivan eritt„in hyvin aineistoon yht„ havain-toa lukuunottamatta.TW10@ Laskemme nyt lineaarisen mallin parametrit jam„„r„„mme samalla mallin j„„nn”kset.TW10@TW10@J„„nn”sTW10@TW5@11.1111TW5@TW5@LINREG FORBES,0RT=1@1@+@C1@=  /  VARS=T(X),LP(Y),J„„nn”s(R)TW20@€TW10@TW50@   Tulokset vaikuttavat melko hyvilt„.TW10@ Mallin selitysosuus on 99.5 %.TW20@   Jos katsotaan j„„nn”ksi„,TW10@TW10@¹TW5@¹¹TW10@TW5@12TW10@TW5@¹TW20@TW5@havaitaan, ett„ joukossa on todella poikkeavahavainto (Nro=12),TW5@ jonka j„„nn”s on huomattavansuuri muihin verrattuna.TW10@   Forbes on huomauttanut t„st„ havainnostamuistiinpanoissaanTW5@ ja pit„„ ilmeisen„, ett„mittauslukemat on t„ss„ tapauksessa otettu v„„rin.TW10@   On silloin perusteltua h„vitt„„ ko. havainto jalaskea tulokset uudelleen.TW10@   Yksinkertaisimmin se tapahtuu poistamalla 12. rivihavaintotaulukosta.TW10@ Periaatteessa kauniimpi tapa onolla t„rvelem„tt„ alkuper„ist„ aineistoaTW5@ ja m„„ritell„keinotekoinen muuttuja I,TW5@ joka saa arvon 0 havainnossa12TW5@ ja arvon 1 muilla havainnoilla.TW10@   Regressioanalyysi tehd„„n sitten ehdolla IND=I .TW20@.±   S„ilytt„„ksemme t„h„nastiset tapahtumat kopioimme aineiston:TW10@³FORBESTW10@¹¹¹TW10@TW10@¹TW10@TW10@2TW10@a,b,n,mTW10@mnabTW10@   M„„rittelemme muuuttujan ITW10@TW10@1TW5@TW5@ITW5@TW5@1TW5@11111111110TW10@11111TW10@ºTW5@ ja teemme regressioanalyysin ilman12. havaintoa:TW10@LINREG FORBES2,0RT=1@1@+@C1@= / VARS=T(X),LP(Y),J„„nn”s(R)  IND=ITW20@€TW10@C.0wWwC.wWwC= w=TW30@   Mallin selitysaste on per„ti T=6@C100@*@5@@6 % eli ep„ilytt„v„n havainnonpoistaminen paransi tarkkuutta.TW10@ Parametreille saatiin arvota==TW5@ ja b=@3TW5@ eli etsitty muuttujien T ja P riippuvuuson ilmaistavissa muodossa 100*lgt(P)=a*T+bTW10@ eli     P=10^((=*T@3)/100) .TW30@   Jos oletetaan,TW5@ ett„ mallin selitysvirheet ovat riippumattomiaTW5@ja normaalisti jakautuneita,TW5@ my”s regressiokertoimien estimaatit(arviot) noudattavat normaalijakaumaa.TW10@ Kun esim. parametrin akeskivirhe on @2,TW10@ saadaan sen 95%:n luottamusv„lin alarajaksiTW5@16-2=14 vapausasteen t-jakauman avullaTW10@=+t.G(14,0.025)*@2=€TW10@ ja yl„rajaksi=+t.G(14,0.975)*@2=€ .TW20@   Ota selville, mit„ nykyisin tiedet„„n veden kiehumispisteenriippuvuudesta ilmanpaineestaTW5@ ja tutki, miten hyvin Forbesin tiedotja tulokset pit„v„t yht„ sen kanssa.TW10@Tp        T„m„ sukro p„„ttyy. Paina ENTER-nappia!@TVC1200@_@@Ta1@BGPLOT /DEL ALL€Ta-1@Tp@0T!1@C4@TLREGS@Tÿ

Ta1@SCRATCH€üGPLOT /DEL ALL€Bü   µµµµ Nis„kk„iden aivojen painot    Tutkimme aineistoa, johon on koottu er„iden nis„kk„iden keski-m„„r„inen paino kiloina ja aivojen keskipaino grammoina. Tarkoituson selvitt„„, millainen on aivojen painon riippuvuus koko painostaja millaiset ovat lajikohtaiset erot.   Otamme t„ss„ esille vain osan aineistosta, jonka T.Allison jaD.V.Cicchetti ker„siv„t 1970-luvulla tutkiessaan varsinaisestinis„kk„iden nukkumista.Tp        Jatka painamalla ENTER!@TVC1200@_@GA@TXA@Tp@Ta-1@DATA AIVOTTW5@ Laji           Paino       AivotTW10@T                   T            T    (sarkainrivi)TW10@ Afr.norsu	6654	5712TW5@ Int.norsu	2547	4603TW5@ Hevonen	521	655Ta1@ Lehm„	465	423 Gorilla	207	406 Sika	192	180 Aasi	187.1	419 Jaguaari	100	157 Harmaahylje	85	325Ta-1@ Ihminen	62	1320TW10@Ta1@ Lammas	55.5	175 Simpanssi	52.160	440 Kenguru	35	56 Vuohi	27.66	115 Pesukarhu	4.288	39.2 Kettu	4.235	50.4 Kissa	3.3	25.6 Rotta	0.28	1.9 Hamsteri	0.12	1 Hiiri	0.023	0.4Ta-1@TW20@   Varustamme taulukon rivitunnuksin ja maskein.TW10@TW5@BTW5@ANMTW5@TW5@ AAAAAAAAAAA   1111.111     1111.1TW10@TW5@,A,B,N,MTW10@TW5@TW5@   Piirr„mme kuvan:TW10@GPLOT AIVOT,Paino,AivotTW10@€TW10@   N„emme, ett„ suurin osa tapauksista kasautuu kuvion vasempaanalakulmaan.TW10@ Vain kaksi lajia (norsut)TW5@ erottuu "massasta".TW10@º  TW10@/  POINT=[PIENI],LajiTW10@€TW10@.±   T„llaisissa tilanteissa, joissa tutkitaan biologisten eli”ideneri osien suhteita,TW5@ on usein havaittu hyv„ksi keinoksi logaritmoidamolemmat muuttujat.TW10@ Teemme uudet muuttujat LogP ja LogA t„llaisinamuunnoksina.TW10@TW10@LogP    LogATW10@TW5@11.111  11.111TW10@TW5@VAR LogP,LogA TO AIVOTTW10@LogP=log(Paino) LogA=log(Aivot)TW10@TW10@€TW10@   Piirr„mme n„m„ muuttujat vastakkain:TW10@GPLOT AIVOT,LogP,LogATW10@€TW10@   Nyt havainnot levitt„ytyv„t melko hyvin koko vaihtelualueelle.TW10@Muuttujien v„lill„ n„ytt„„ my”s olevan melko selv„ lineaarinenriippuvuussuhde.TW10@ºTW5@ / TREND=0TW10@€TW10@.±   Tutkimme riippuvuutta tarkemmin lineaarisella mallilla,TW5@ jossaLogA on selitett„v„ muuttujaTW5@ ja LogP selitt„v„ muuttuja.TW10@ Laskemme my”sj„„nn”kset.TW10@TW10@J„„nn”sTW10@TW5@11.111TW10@TW5@LINREG AIVOT,0RT=1@1@+@C1@=  /  VARS=LogP(X),LogA(Y),J„„nn”s(R)  RESULTS=0TW20@€TW50@   Malli tuntuu toimivan hyvin.TW10@ Katsokaamme kuitenkin j„„nn”ksi„.TW10@Tp        Kopioimme j„„nn”kset!@TW10@TW10@Tp@TW10@¹TW5@¹TW5@TW5@¹TW10@TW5@¹TW10@TW5@TW5@TW5@TW5@TW5@DELETE36TW10@€TW10@TW10@Malliin n„hden "ylipainoiset" aivot ovatennen muuta ihmisell„ ja jossain m„„rinmy”s simpanssilla.TW10@Selvin negatiivinen j„„nn”s on sialla,TW5@mutta se johtunee enemm„n raskaastaruumiinrakenteesta kuin aivojen kehitty-m„tt”myydest„.TW10@ Sama koskee my”s lehm„„.TW10@.±   Selvitt„„ksemme, kuinka poikkeuksellinen ihminen on t„ss„ joukossa,TW5@muodostamme apumuuttujan I,TW5@ joka saa arvon 1 ihmisell„ ja 0 muillalajeilla.TW10@ Otamme t„m„n muuttujan toiseksi selitt„j„ksiTW5@ eli laskemmemallifunktion a*LogP+b*I+c parametrien a,b,c estimaatit LINREG-operaatiolla.TW20@TW10@ITW5@TW5@1TW10@TW5@VAR I=0 TO AIVOTTW10@€TW10@TW5@TW5@TW10@1TW30@TW10@LINREG AIVOT,0RT=1@1@+@C1@= / VARS=LogP(X),I(X),LogA(Y)  RESULTS=0TW10@€TW30@C.WW0wµµµµµ     TW10@   Tulostaulukossa on valmiina regressiokertoimien t-arvotTW5@ eli kertoimien ja niiden keskivirheiden suhteet.TW10@ Jos kerroin todellisuudessaon 0,TW5@ t noudattaa t-jakaumaa 17 vapausasteellaTW10@ (17 on havaintojen m„„r„20 v„hennettyn„ estimoitujen parametrien m„„r„ll„ 3).TW10@   T„ss„ tapauksessa "ihmisyys"-muuttujan I regressiokertoimen t-arvoon = .TW10@ Sen merkitsevyystasoksi saadaan t.F(17,=)=€TW20@eli selitt„j„ I tuo olennaista lis„tietoa malliin.TW10@ Alkuper„inen malliei n„in riit„ selitt„m„„n ihmisen aivojen kokoa.TW20@Tp        T„m„ sukro p„„ttyy. Paina ENTER-nappia!@TVC1200@_@@Ta1@BGPLOT /DEL ALL€Ta-1@Tp@0T!1@C5@TLREGS@Tÿ

Ta1@SCRATCH €   µµµµ Satunnaispitoinen p„„ttelyteht„v„    Olemme synnytt„neet alla olevan taulukon seuraavasti. KokonaisluvutX=1,2,...,10 on kukin kerrottu kokonaisluvulla K ja tulokseen on lis„ttytoinen kokonaisluku L sek„ viel„ nopanheitolla saatu pistem„„r„ elilopullinen tulos Y on muotoa K*X+L+(nopanheiton_tulos).   Teht„v„si on p„„tell„, mitk„ ovat kokonaisluvut K ja L.  X   Y  1  16  2  25  3  27  4  30  5  35  6  44  7  47  8  52  9  59 10  60   Yrit„ ratkaista teht„v„ itseksesi. Katso vasta sen j„lkeen t„ss„esitetty ratkaisu.Tp        Jatka painamalla ENTER! Keskeyt„ painamalla . (piste).@TVC1200@_@GA@TXA@Tp@Ta-1@   Ratkaisemme teht„v„n regressioanalyysilla.TW10@ Annamme aineistolle nimenKOE.TW10@TW5@TW5@DATA KOETW10@ºTW10@ Laskemme regressiomallin,TW5@ jonka selitett„v„ muuttuja on Y jaselitt„j„ X.TW10@TW5@LINREG KOE,0RT=1@1@+@C1@= / VARS=X(X),Y(Y)  RESULTS=0TW10@€TW30@C.0wwTW30@   On syyt„ uskoa, ett„ muuttujan X regressiokerroin = onl„hell„ etsitty„ kerrointa K,TW5@ joka lienee siis 5.TW10@ Vakiotermi@2 taas on tekemisiss„ lis„tyn vakion L kanssa.TW10@ Sille saadaanarvio,TW5@ kun vakiotermist„ v„hennet„„n nopanheiton teoreettinenkeskiarvo (1+2+3+4+5+6)/6=€ºTW10@ eli @2-3.5=€TW10@º viittaa siihen,TW5@ ett„L saattaisi olla 9.TW10@ Tarkastamme tuloksen laskemalla taulukkoon uudensarakkeen U=Y-5*X-9,TW10@ jonka pit„isi kertoa, mit„ saatiin kullakinnopanheitolla.TW20@TW5@TW5@ºTW5@,A,B,N,MTW10@ 111111UTW10@MNABTW10@VAR U=Y-5*X-9 TO KOETW10@€TW30@TW10@Koska kaikki U-arvot ovat mahdollisia nopanheitontuloksia,TW5@ ratkaisu K=5, L=9 on kelvollinen.TW10@Muita hyv„ksytt„vi„ ratkaisuja ei t„ss„ tapauksessal”ydy.TW10@ Sen huomaa esim. kokeilemalla muita t„t„l„hell„ olevia arvoyhdistelmi„.TW10@ Ne antavat U-arvoja,TW5@jotka ovat alle 1 tai yli 6.ºTW20@Tp        T„m„ sukro p„„ttyy. Paina ENTER-nappia!@TVC1200@_@GB@TXB@Tp@0T!1@C6@TLREGS@TÿTa1@SCRATCH€üGPLOT /DEL ALL€Bü   µµµµ Polynomiregressio    Tarkastelemme edelleen kahden muuttujan X ja Y riippuvuutta,mutta nyt oletetaan regressiofunktion olevan X:n polynomieli muotoa a0+a1*X+a2*X^2+...+ak*X^k .   Kun riippuvuus n„ytt„„ k„yr„viivaiselta tai on olemassa teoria,joka viittaa sellaiseen, on syyt„ kokeilla polynomiregressiota.   Jotta LINREG-operaatiota voi k„ytt„„ polynomiregressiossa, jokainentarvittava X:n potenssi on m„„ritelt„v„ ja laskettava valmiiksihavaintoaineistoon omana muuttujanaan.   Kuten yleisestikin usean selitt„v„n muuttujan regressiossa, jokainenselitt„v„ muuttuja aktivoidaan X:ll„.   Mallin parametrit a0,a1,a2,...,ak lasketaan edelleen pienimm„nneli”summan keinolla.   Kuvaamme t„ss„ polynomiregressiota simuloidulla aineistolla, jokanoudattaa 3. asteen polynomimallia normaalijakautuneella virhetermill„.Sovitamme aineistoon vaiheittain regressiosuoran (1. asteen) ja sitten2. ja 3. asteen polynomin.Tp        Jatka painamalla ENTER!@TVC1200@_@GA@TXA@Tp@.±Ta-1@   Asetamme aineistolle kehykset ja m„„rittelemme selitt„j„n X:TW10@DATA POLY,A,B,N,MTW10@MNA11X12345678910BY1111TW20@   Luomme Y-arvot operaatiollaTW5@VAR Y=5000+100*X-60*X^2+5*X^3+20*probit(rnd(1)) TO POLYTW10@eli "oikeat" regressiokertoimet ovat a0=5000, a1=100, a2=-60 ja a3=6.TW20@Riippuvuutta h„iritsee virhetermi, joka on normaalijakautunut keski-arvolla 0 ja hajonnalla 20.TW20@TW10@€TW20@.±   Kokeilemme ensin tavallista lineaarista regressiota:TW10@LINREG POLY,0RT=1@1@+@C1@= / VARS=X(X),Y(Y) RESULTS=0TW20@€TW10@TW50@   Tulos on surkea, mik„ ilmenee kuvastakin:TW10@GPLOT POLY,X,Y / TREND=0 HEADER=  OUTFILE=ATW10@€TW10@.±   Polynomiregressiota varten laskemme muuttujan X toiset ja kolmannetpotenssit aineistoon POLY muuttujina X2 ja X3.TW10@X2    X3TW5@1111  1111TW10@TW10@VAR X2,X3 TO POLYTW10@X2=X^2 X3=X^3TW5@TW10@€TW10@Tp        Kurkistamme taaksep„in n„hd„ksemme tuloksen!@TW20@Tp@TW10@.±   Koska aineistolla n„ytt„„ olevan "k„„nnepiste"TW5@ ja sek„ "maksimi" ett„"minimi",TW5@ on selv„„, ettei 2. asteen mallikaan riit„.TW10@ Kokeilemme sit„kuitenkin.TW10@LINREG POLY,0RT=1@1@+@C1@= / VARS=X(X),X2(X),Y(Y) RESULTS=0TW10@€TW10@C.0wwwC=C= w=TW30@   Selitysaste on noussut T=4@C100@*@4@@4 prosenttiin,TW5@ mutta malli on siltipuutteellinen,TW5@ mik„ n„kyy hyvin kuvasta:TW10@.±GPLOT Y(X)=@3=*X+@2*X^2TW20@XSCALE=0(1)10 YSCALE=4400(200)5200 HEADER=  INFILE=A OUTFILE=ATW10@TW10@€TW10@.±TW5@   Siirrymme siis kolmannen asteen polynomimalliin.TW10@LINREG POLY,0RT=1@1@+@C1@= / VARS=X(X),X2(X),X3(X),Y(Y)  RESULTS=0TW20@€TW30@C.0wwwwC=C= w=TW10@   Malli on selv„sti loksahtanut paikoilleen,TW5@ mist„ puhuu esim.eritt„in korkea selitysaste T=5@C100@*@5@@5%.TW10@ Parametrien estimaatit eiv„tsen sijaan ole kovin tarkkoja (vertaa "oikeisiin"),TW5@ mit„ osoittavatkeskivirheetkin.TW10@ Malli mukautuu kuitenkin eritt„in hyvin aineistoonkuvan perusteella:TW10@.±GPLOT Y(X)=@4+=*X@2*X^2+@3*X^3TW10@XSCALE=0(1)10 YSCALE=4400(200)5200 HEADER=  INFILE=A OUTFILE=ATW10@TW10@€TW10@.±   Vertailun vuoksi piirr„mme kuvaan viel„ "oikean" regressiofunktion:TW10@GPLOT Y(X)=5000+100*X-60*X^2+5*X^3TW10@±CUR-4TW10@X=[PUNAINEN],0,10TW10@TW10@€TW10@   Se yhtyy aineistoon ja estimoituun regressiok„yr„„n muualla hyvinpaitsi ensimm„isess„ pisteess„,TW5@ jota ilmeisesti rasittaa suhteellisensuuri virhetermi.TW20@.±Ta1@   T„m„n kokeen er„s opetus on, ett„ vaikka mallin selitysosuus onvarsin hyv„, parametriestimaattien ei tarvitse olla kovin tarkkoja.Syyn„ on se, ett„ selitt„j„t X, X^2 ja X^3 korreloivat kesken„„neritt„in voimakkaasti (tarkista!), jolloin ne ik„„nkuin kilpailevatkesken„„n. T„ll”in suurin piirtein yht„ hyvi„ selityksi„ voi l”yty„hyvin erilaisillakin parametriyhdistelmill„.Ta-1@TW20@Tp        T„m„ sukro p„„ttyy. Paina ENTER-nappia!@TVC1200@_@@Ta1@BGPLOT /DEL ALL€Ta-1@Tp@0T!1@C7@TLREGS@Tÿ

Ta1@SCRATCH€   µµµµ Kymmenottelun yhteispistem„„r„n ennustaminen    Esimerkkin„ aidosta usean muuttujan regressioanalyysista tarkaste-lemme kymmenottelijoita kuvaavaa aineistoa KYMMEN. Tutkimme, mitenhyvin on mahdollista ennustaa lopullinen yhteispistem„„r„ esim.vain kahden ensimm„isen lajin tai ensimm„isen p„iv„n viiden lajinyhteispisteiden perusteella.   Jos kymmenottelijoiden suoritukset eri lajeissa olisivat toisistaant„ysin riippumattomia, olisi ilmeist„, ett„ 2 ensimm„ist„ lajiaselitt„„ noin 20% ja ensimm„inen p„iv„ noin 50% lopputuloksesta.Lajit ovat kuitenkin aika lailla korreloituneita, joten on odotetta-vissa parempia ennusteita.>COPY KYMMEN KYMMEN.SVO€Tp        Jatka painamalla ENTER!@TVC1200@_@GA@TXA@Tp@.±Ta-1@   Laskemme aluksi kaikkien kiinnostuksen kohteena olevien muuttujienkorrelaatiot.TW10@ Valitaan muuttujat:TW5@FILE ACTIVATE KYMMEN€TD1@---TD2@²TD0@TW10@CORR KYMMEN,0RT=1@1@+@C1@=TW10@€TW30@TW20@TW5@¹TW5@¹TW5@TW5@¹TW10@TW30@TW5@¹TW20@º   Havaitsemme, ett„ parhaiten korreloi yhteis-pisteiden kanssa eri lajeista pituushyppy.TW10@Koska korrelaatiokerroin on noin 0.5,TW5@ seyksin selitt„„ lopputuloksesta 25%.TW20@Muita hyvi„ ennustajia ovat 110 m aidat jakiekonheitto.TW10@Huonoiten ennustaa t„ss„ aineistossa 1500 mjuoksu,TW5@ jossa menestyminen n„in "huonontaa"loppupistem„„r„„.TW10@ Kerroin -0.15 ei kuitenkaanole tilastollisesti merkitsev„, sill„TW10@/RMIN 0.05,48TW10@€TW30@.±   Siirrymme nyt regressioanalyysiin.TW10@ Laskiessamme erilaisia „skeisiinmuuttujiin liittyvi„ malleja,TW5@ voimme k„ytt„„ hyv„ksi valmiina oleviakeskiarvoja, hajontoja ja korrelaatioita.TW10@ Keskiarvot ja hajonnat ovatmatriisitiedostossa MSN.MTW5@ ja korrelaatiokertoimet matriisitiedostossaCORR.M  CORR-operaation j„lkeen.TW20@   T„ll”in LINREG-operaatiossa viitataan pelk„n havaintoaineistonKYMMEN asemasta my”s n„ihin v„lituloksiin merkinn„ll„ KYMMEN>M .TW20@Jos havaintoja on hyvin paljonTW5@ (t„ss„ ei ole),TW5@ laskenta-aika lyheneeratkaisevasti,TW5@ koska koko aineistoa ei tarvitse k„yd„ l„pi jokamallilla erikseen.TW10@   Koska havainnot ovat tiedostossa,TW5@ mallin valinta tapahtuu helpoitenFILE ACTIVATE -komennolla tai alt-F6 -napilla.TW10@   Otamme ensimm„iseen malliin selitt„jiksi vain kaksi ensimm„ist„lajia:TW10@FILE ACTIVATE KYMMENTW10@€TD1@YXX--------TD2@²TD0@TW10@ja laskemme mallinTW5@LINREG KYMMEN>M,0RT=1@1@+@C1@= / RESULTS=0TW20@€TW10@TW10@   Kaksi ensimm„ist„ lajia (100 m ja pituushyppy) selitt„v„t yhteis-pisteist„ noin 29% TW5@j„lkimm„isen ollessa selv„sti parempi selitt„j„.TW10@T„m„ on jonkin verran enemm„n kuin 2/10 eli 20%.TW10@Vakiotermi (6120) kertoo,TW5@ kuinka paljon pisteit„ j„„ keskim„„rinmuiden lajien osalle.TW10@.±   Voisimme seurata tilannetta laji lajiltaTW5@ (tee se!),TW5@ mutta hypp„„mmenyt suoraan viiden ensimm„isen lajinTW5@ (ensimm„isen ottelup„iv„n)j„lkeiseen vaiheeseen,TW5@ joka lienee mielenkiintoisin.TW10@FILE ACTIVATE KYMMENTW10@€TD1@XXXTD2@²TD0@TW10@LINREG KYMMEN>M,0RT=1@1@+@C1@= / RESULTS=0TW10@€TW10@TW30@   Selitysosuus on kasvanut l„hes 64 prosenttiin.TW10@ Se on kuitenkin niinkaukana sadasta,TW5@ ett„ j„nnityst„ riitt„„ toiselle p„iv„llekin.TW20@.±Ta1@   Jos lasket mallin, jossa kaikki 10 lajia ovat selitt„jin„, pit„isisynty„ t„ydellinen selitys, jossa kertoimet ovat ykk”si„ ja vakio-termi on 0. N„in ei t„ll„ aineistolla aivan k„y, sill„ sen tallennuk-sessa on joitakin pieni„ virheit„. Yhteispisteet eiv„t t„sm„„ kaikkienottelijoiden kohdalla.Ta-1@TW50@FILE DEL KYMMEN€TW20@Tp        T„m„ sukro p„„ttyy. Paina ENTER-nappia!@TVC1200@_@GB@TXB@Tp@0T!1@CX@TLREGS@Tÿ